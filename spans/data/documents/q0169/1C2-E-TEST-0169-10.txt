lu(B(m,m))
636
Even though this is a small example, the results are typical. The original numbering scheme leads to the most fill-in. The fill-in for the reverse Cuthill-McKee ordering is concentrated within the band, but it is almost as extensive as the first two orderings. For the approximate minimum degree ordering, the relatively large blocks of zeros are preserved during the elimination and the amount of fill-in is significantly less than that generated by the other orderings. The spy plots below reflect the characteristics of each reordering.
Cholesky Factorization
If S is a symmetric (or Hermitian), positive definite, sparse matrix, the statement below returns a sparse, upper triangular matrix R so that R'*R = S.
R = chol(S)
chol does not automatically pivot for sparsity, but you can compute approximate minimum degree and profile limiting permutations for use with chol(S(p,p)).
Since the Cholesky algorithm does not use pivoting for sparsity and does not require pivoting for numerical stability, chol does a quick calculation of the amount of memory required and allocates all the memory at the start of the factorization. You can use symbfact , which uses the same algorithm as chol, to calculate how much memory is allocated.
QR Factorization
MATLAB computes the complete QR factorization of a sparse matrix S with
[Q,R] = qr(S)
or
[Q,R,E] = qr(S)
but this is often impractical. The unitary matrix Q often fails to have a high proportion of zero elements. A more practical alternative, sometimes known as "the Q-less QR factorization," is available.
With one sparse input argument and one output argument
R = qr(S)
returns just the upper triangular portion of the QR factorization. The matrix R provides a Cholesky factorization for the matrix associated with the normal equations:
R'*R = S'*S
However, the loss of numerical information inherent in the computation of S'*S is avoided.
With two input arguments having the same number of rows, and two output arguments, the statement
[C,R] = qr(S,B)
applies the orthogonal transformations to B, producing C = Q'*B without computing Q.
The Q-less QR factorization allows the solution of sparse least squares problems
with two steps
[c,R] = qr(A,b) x = R\c
If A is sparse, but not square, MATLAB uses these steps for the linear equation solving backslash operator:
x = A\b
Or, you can do the factorization yourself and examine R for rank deficiency.
It is also possible to solve a sequence of least squares linear systems with different right-hand sides, b, that are not necessarily known when R = qr(A) is computed. The approach solves the "semi-normal equations"
R'*R*x = A'*b
with
and then employs one step of iterative refinement to reduce round off error:
r = b - A*x e = R\(R'\(A'*r)) x = x + e
Incomplete Factorizations
The ilu and ichol functions provide approximate, incomplete factorizations, which are useful as preconditioners for sparse iterative methods.
The ilu function produces three incomplete lower-upper (ILU) factorizations: the zero-fill ILU (ILU(0)), a Crout version of ILU (ILUC(tau)), and ILU with threshold dropping and pivoting (ILUTP(tau)).  The ILU(0) never pivots and the resulting factors only have nonzeros in positions where the input matrix had nonzeros.  Both ILUC(tau) and ILUTP(tau), however, do threshold-based dropping with the user-defined drop tolerance tau.
For example:
A = gallery('neumann', 1600) + speye(1600); nnz(A) ans =         7840  nnz(lu(A)) ans =       126478
shows that A has 7840 nonzeros, and its complete LU factorization has 126478 nonzeros. On the other hand, the following code shows the different ILU outputs:
[L,U] = ilu(A); nnz(L)+nnz(U)-size(A,1); ans =         7840  norm(A-(L*U).*spones(A),'fro')./norm(A,'fro') ans =   4.8874e-017  opts.type = 'ilutp'; opts.droptol = 1e-4; [L,U,P] = ilu(A, opts); nnz(L)+nnz(U)-size(A,1) ans =        31147  norm(P*A - L*U,'fro')./norm(A,'fro') ans =   9.9224e-005  opts.type = ‘crout'; nnz(L)+nnz(U)-size(A,1) ans =        31083 norm(P*A-L*U,'fro')./norm(A,'fro') ans =   9.7344e-005
These calculations show that the zero-fill factors have 7840 nonzeros, the ILUTP(1e-4) factors have  31147 nonzeros, and the ILUC(1e-4) factors have 31083 nonzeros.  Also, the relative error of the product of the zero-fill factors is essentially zero on the pattern of A.  Finally, the relative error in the factorizations produced with threshold dropping is on the same order of the drop tolerance, although this is not guaranteed to occur.  See the ilu reference page for more options and details.
The ichol function provides zero-fill incomplete Cholesky factorizations (IC(0)) as well as threshold-based dropping incomplete Cholesky factorizations (ICT(tau)) of symmetric, positive definite sparse matrices. These factorizations are the analogs of the incomplete LU factorizations above and have many of the same characteristics.  For example:
A = delsq(numgrid('S',200)); nnz(A) ans =       195228  nnz(chol(A,'lower')) ans =      7762589
shows that A has 195228 nonzeros, and its complete Cholesky factorization without reordering has 7762589 nonzeros.  By contrast:
L = ichol(A); nnz(L) ans =       117216 norm(A-(L*L').*spones(A),'fro')./norm(A,'fro') ans =   3.5805e-017  opts.type = 'ict'; opts.droptol = 1e-4; L = ichol(A,opts); nnz(L) ans =      1166754  norm(A-L*L','fro')./norm(A,'fro') ans =   2.3997e-004
IC(0) has nonzeros only in the pattern of the lower triangle of A, and on the pattern of A, the product of the factors matches.  Also, the ICT(1e-4) factors are considerably sparser than the complete Cholesky factor, and the relative error between A and L*L' is on the same order of the drop tolerance.  It is important to note that unlike the factors provided by chol, the default factors provided by ichol are lower triangular.  See the ichol reference page for more information.
Systems of Linear Equations
There are two different classes of methods for solving systems of simultaneous linear equations:
Direct methods are usually variants of Gaussian elimination. These methods involve the individual matrix elements directly, through matrix operations such as LU or Cholesky factorization. MATLAB implements direct methods through the matrix division operators / and \, which you can use to solve linear systems.
Iterative methods produce only an approximate solution after a finite number of steps. These methods involve the coefficient matrix only indirectly, through a matrix-vector product or an abstract linear operator. Iterative methods are usually applied only to sparse matrices.
Direct Methods
Direct methods are usually faster and more generally applicable than indirect methods, if there is enough storage available to carry them out. Iterative methods are usually applicable to restricted cases of equations and depend on properties like diagonal dominance or the existence of an underlying differential operator. Direct methods are implemented in the core of the MATLAB software and are made as efficient as possible for general classes of matrices. Iterative methods are usually implemented in MATLAB-language files and can use the direct solution of subproblems or preconditioners.
Using a Different Preordering.  If A is not diagonal, banded, triangular, or a permutation of a triangular matrix, backslash (\) reorders the indices of A to reduce the amount of fill-in—that is, the number of nonzero entries that are added to the sparse factorization matrices. The new ordering, called a preordering, is performed before the factorization of A. In some cases, you might be able to provide a better preordering than the one used by the backslash algorithm.
To use a different preordering, first turn off both of the automatic preorderings that backslash might perform by default, using the function spparms as follows:
defaultParms = spparms('autoamd',0); spparms('autommd',0);
Now, assuming you have created a permutation vector p that specifies a preordering of the indices of A, apply backslash to the matrix A(:,p), whose columns are the columns of A, permuted according to the vector p.
x = A (:,p) \ b; x(p) = x; spparms(defaultParms);
The command spparms(defaultParms) restores the controls to their prior state, in case you use A\b later without specifying an appropriate preordering.
Iterative Methods
Eleven functions are available that implement iterative methods for sparse systems of simultaneous linear systems.
Functions for Iterative Methods for Sparse Systems
Function
