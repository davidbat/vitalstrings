Posted 12 June 2010 - 10:30 PM
AlexanderAgathos, on Jun 12 2010, 08:39 PM, said:
I think that you do not really mean the exponential of a Matrix as a power series but just the exponential function on its elements.
Yes, I mean performing the exponential on each element in the matrix.
AlexanderAgathos, on Jun 12 2010, 08:39 PM, said:
There is a sparce matrix to vector multiplication one can simply regard a dense matrix multiplication with a sparce matrix as a series of vector and sparce matrix multiplication so:
Yes, I was thinking of performing the matrix-vector operation multiple times.
AlexanderAgathos, on Jun 12 2010, 08:39 PM, said:
Also in activation=f_logistic(0-summation-C[j]); do you really mean this or do you mean : -summation-C[i*num_hid+j] why do you use the zero it is a useless addition.
I wrote 0-X, but I guess what I really mean is to just negate the X matrix...
AlexanderAgathos, on Jun 12 2010, 08:39 PM, said:
One nice exercise can be to feed the whole matrix to the kernel and do it inide the kernel.
There are two solutions here on this:
One is to do a for loop inside the kernel or unfold the matrix in a vector and do in parallel all the calculations. The second is fast the first is slow. You should see a huge speedup of less than a second in your dimensions.
I'm looking at modifying the standard NVIDIA matrix multiplier function for now (it is not sparse).
/*  * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.  *  * NVIDIA Corporation and its licensors retain all intellectual property and   * proprietary rights in and to this software and related documentation.   * Any use, reproduction, disclosure, or distribution of this software   * and related documentation without an express license agreement from  * NVIDIA Corporation is strictly prohibited.  *  * Please refer to the applicable NVIDIA end user license agreement (EULA)   * associated with this source code for terms and conditions that govern   * your use of this NVIDIA software.  *   */  /* Matrix multiplication: C = A * B.  * Device code.  */  #ifndef _MATRIXMUL_KERNEL_H_ #define _MATRIXMUL_KERNEL_H_  #include <stdio.h> #include "matrixMul.h" #define CHECK_BANK_CONFLICTS 0 #if CHECK_BANK_CONFLICTS #define AS(i, j) cutilBankChecker(((float*)&As[0][0]), (BLOCK_SIZE * i + j)) #define BS(i, j) cutilBankChecker(((float*)&Bs[0][0]), (BLOCK_SIZE * i + j)) #else #define AS(i, j) As[i][j] #define BS(i, j) Bs[i][j] #endif  ///////////////////////////////////////////////////////////// /////////////////// //! Matrix multiplication on the device: C = A * B //! wA is A's width and wB is B's width ///////////////////////////////////////////////////////////// /////////////////// __global__ void matrixMul( float* C, float* A, float* B, int wA, int wB) {     // Block index     int bx = blockIdx.x;     int by = blockIdx.y;      // Thread index     int tx = threadIdx.x;     int ty = threadIdx.y;      // Index of the first sub-matrix of A processed by the block     int aBegin = wA * BLOCK_SIZE * by;      // Index of the last sub-matrix of A processed by the block     int aEnd   = aBegin + wA - 1;      // Step size used to iterate through the sub-matrices of A     int aStep  = BLOCK_SIZE;      // Index of the first sub-matrix of B processed by the block     int bBegin = BLOCK_SIZE * bx;      // Step size used to iterate through the sub-matrices of B     int bStep  = BLOCK_SIZE * wB;      // Csub is used to store the element of the block sub-matrix     // that is computed by the thread     float Csub = 0;      // Loop over all the sub-matrices of A and B     // required to compute the block sub-matrix     for (int a = aBegin, b = bBegin;              a <= aEnd;              a += aStep, b += bStep) {          // Declaration of the shared memory array As used to         // store the sub-matrix of A         __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];          // Declaration of the shared memory array Bs used to         // store the sub-matrix of B         __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];          // Load the matrices from device memory         // to shared memory; each thread loads         // one element of each matrix         AS(ty, tx) = A[a + wA * ty + tx];         BS(ty, tx) = B[b + wB * ty + tx];          // Synchronize to make sure the matrices are loaded         __syncthreads();          // Multiply the two matrices together;         // each thread computes one element         // of the block sub-matrix         for (int k = 0; k < BLOCK_SIZE; ++k)             Csub += AS(ty, k) * BS(k, tx);          // Synchronize to make sure that the preceding         // computation is done before loading two new         // sub-matrices of A and B in the next iteration         __syncthreads();     }      // Write the block sub-matrix to device memory;     // each thread writes one element     int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;     C[c + wB * ty + tx] = Csub; }  #endif // #ifndef _MATRIXMUL_KERNEL_H_  int main(int argc,char *argv[]) {  return 0; }
with particular attention to this line:
Csub += AS(ty, k) * BS(k, tx);
Not to mention all the other little bits of code to set up the blocks correctly.
Here is the above .cu's header file:
/*  * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.  *  * NVIDIA Corporation and its licensors retain all intellectual property and   * proprietary rights in and to this software and related documentation.   * Any use, reproduction, disclosure, or distribution of this software   * and related documentation without an express license agreement from  * NVIDIA Corporation is strictly prohibited.  *  * Please refer to the applicable NVIDIA end user license agreement (EULA)   * associated with this source code for terms and conditions that govern   * your use of this NVIDIA software.  *   */  #ifndef _MATRIXMUL_H_ #define _MATRIXMUL_H_  // Thread block size #define BLOCK_SIZE 16  // Basic Matrix dimensions (can be amplified by command line switch) // (chosen as multiples of the thread block size for simplicity) #define WA (5  * BLOCK_SIZE) // Matrix A width #define HA (10 * BLOCK_SIZE) // Matrix A height #define WB (5  * BLOCK_SIZE) // Matrix B width #define HB WA  // Matrix B height #define WC WB  // Matrix C width  #define HC HA  // Matrix C height  #endif // _MATRIXMUL_H_
I've got it all compiling now. I guess I just need to modify the .cu file to accomodate a C matrix, negate a few variables and incorporate the logistic operation. If I can get it working in dense mode, I expect to see a considerable speed improvement. TBQH, I'm not sure if I'll even bother with the sparse version if I can get my compute time down from a month to a day, then again, if I can get another 20%, it'd be worth it!
AlexanderAgathos, on Jun 12 2010, 08:39 PM, said:
Hope that helps.
Most definitely. Thanks so much for your insight.
This post has been edited by HickoryDock: 12 June 2010 - 10:32 PM
