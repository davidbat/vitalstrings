1
Fig. 2. The buckyball as rendered by spy and gplot.
As this illustrates, sparse matrices are printed as a list of their nonzero elements (with indices), in column major order. The function nnz(A) returns the number of nonzero elements of A. It is implemented by scanning full matrices, and by access to the internal data structure for sparse matrices. The function nzmax(A) returns the number of storage locations for nonzeros allocated for A. Graphic visualization of the structure of a sparse matrix is often a useful tool. The function spy(A) plots a silhouette of the nonzero structure of A. Figure 1 illustrates such a plot for a matrix that comes from a nite element mesh due to David Eppstein. A picture of the graph of a matrix is another way to visualize its structure. Laying out an arbitrary graph for display is a hard problem that we do not address. However, some sparse matrices (from nite element applications, for example) have spatial coordinates associated with their rows or columns. If xy contains such coordinates for matrix A, the function gplot(A,xy) draws its graph. The second plot in Figure 1 shows the graph of the sample matrix, which in this case is just the same as the nite element mesh. Figure 2 is another example: The spy plot is the 60 60 adjacency matrix of the graph of a Buckminster Fuller geodesic dome, a soccer ball, and a C60 molecule, and the gplot shows the graph itself. Section 3.3.4 describes another function for visualizing the elimination tree of a matrix. 2.4. Creating sparse matrices. Usually one wants to create a sparse matrix directly, without rst having a full matrix A and then converting it with S = sparse(A). One way to do this is by simply supplying a list of nonzero entries and their indices. Several alternate forms of sparse (with more than one argument) allow this. The most general is
S = sparse(i,j,s,m,n,nzmax)
Ordinarily, i and j are vectors of integer indices, s is a vector of real or complex entries, and m, n, and nzmax are integer scalars. This call generates an m n sparse matrix, having one nonzero for each entry in the vectors i, j, and s, with S(i(k) j(k)) = s(k), and with enough space allocated for S to have nzmax nonzeros. The indices in i and j need not be given in any particular order.
5
If a pair of indices occurs more than once in i and j, sparse adds the corresponding values of s together. Then the sparse matrix S is created with one nonzero for each nonzero in this modi ed vector s. The argument s and one of the arguments i and j may be scalars, in which case they are expanded so that the rst three arguments all have the same length. There are several simpli cations of the full six-argument call to sparse. S = sparse(i,j,s,m,n) uses nzmax = length(s). S = sparse(i,j,s) uses m = max(i) and n = max(j). S = sparse(m,n) is the same as S = sparse( ], ], ],m,n), where ] is Matlab's empty matrix. It produces the ultimate sparse matrix, an m n matrix of all zeros. Thus for example
S = sparse( 1 2 3], 3 1 2], 11 22 33])
produces the sparse matrix S from the example in Section 2.3, but does not generate any full 3 by 3 matrix during the process. Matlab's function k = find(A) returns a list of the positions of the nonzeros of A, counting in column-major order. For sparse Matlab we extended the de nition of find to extract the nonzero elements together with their indices. For any matrix A, full or sparse, i,j,s] = find(A) returns the indices and values of the nonzeros. (The square bracket notation on the left side of an assignment indicates that the function being called can return more than one value. In this case, find returns three values, which are assigned to the three separate variables i, j, and s.) For example, this dissects and then reassembles a sparse matrix:
i,j,s] = find(S) m,n] = size(S) S = sparse(i,j,s,m,n)
So does this, if the last row and column have nonzero entries:
i,j,s] = find(S) S = sparse(i,j,s)
Another common way to create a sparse matrix, particularly for nite di erence computations, is to give the values of some of its diagonals. Two functions diags and blockdiags can create sparse matrices with speci ed diagonal or block diagonal structure. There are several ways to read and write sparse matrices. The Matlab save and load commands, which save the current workspace or load a saved workspace, have been extended to accept sparse matrices and save them e ciently. We have written a Fortran utility routine that converts a le containing a sparse matrix in the Harwell-Boeing format 6] into a le that Matlab can load. 2.5. The results of sparse operations. What is the result of a Matlab operation on sparse matrices? This is really two fundamental questions: what is the value of the result, and what is its storage class? In this section we discuss the answers that we settled on for those questions. A function or subroutine written in Matlab is called an m- le. We want it to be possible to write m- les that produce the same results for sparse and for full inputs. Of course, one could ensure this by converting all inputs to full, but that would defeat the goal of e ciency. A better idea, we decided, is to postulate that
6
The value of the result of an operation does not depend on the storage class of the operands, although the storage class of the result may. The only exception is a function to inquire about the storage class of an object: issparse(A) returns 1 if A is sparse, 0 otherwise. Some intriguing notions were ruled out by our postulate. We thought, for a while, that in cases such as A ./ S (which denotes the pointwise quotient of A and S) we ought not to divide by zero where S is zero, since that would not produce anything useful instead we thought to implement this as if it returned A(i j)=S(i j) wherever S(i j) 6= 0, leaving A unchanged elsewhere. All such ideas, however, were dropped in the interest of observing the rule that the result does not depend on storage class. The second fundamental question is how to determine the storage class of the result of an operation. Our decision here is based on three ideas. First, the storage class of the result of an operation should depend only on the storage classes of the operands, not on their values or sizes. (Reason: it's too risky to make a heuristic decision about when to sparsify a matrix without knowing how it will be used.) Second, sparsity should not be introduced into a computation unless the user explicitly asks for it. (Reason: the full matrix user shouldn't have sparsity appear unexpectedly, because of the performance penalty in doing sparse operations on mostly nonzero matrices.) Third, once a sparse matrix is created, sparsity should propagate through matrix and vector operations, concatenation, and so forth. (Reason: most m- les should be able to do sparse operations for sparse input or full operations for full input without modi cation.) Thus full inputs always give full outputs, except for functions like sparse whose purpose is to create sparse matrices. Sparse inputs, or mixed sparse and full inputs, follow these rules (where S is sparse and F is full): Functions from matrices to scalars or xed-size vectors, like size or nnz, always return full results. Functions from scalars or xed-size vectors to matrices, like zeros, ones, and eye, generally return full results. Having zeros(m,n) and eye(m,n) return full results is necessary to avoid introducing sparsity into a full user's computation there are also functions spzeros and speye that return sparse zero and identity matrices. The remaining unary functions from matrices to matrices or vectors generally return a result of the same storage class as the operand (the main exceptions are sparse and full). Thus, chol(S) returns a sparse Cholesky factor, and diag(S) returns a sparse vector (a sparse m 1 matrix). The vectors returned by max(S), sum(S), and their relatives (that is, the vectors of column maxima and column sums respectively) are sparse, even though they may well be all nonzero. Binary operators yield sparse results if both operands are sparse, and full results if both are full. In the mixed case, the result's storage class depends on the operator. For example, S + F and F \ S (which solves the linear system SX = F ) are full S .* F (the pointwise product) and S & F are sparse. A block matrix formed by concatenating smaller matrices, like A B C D
7
is written as A B C D] in Matlab. If all the inputs are full, the result is full, but a concatenation that contains any sparse matrix is sparse. Submatrix indexing on the right counts as a unary operator A = S(i,j) produces a sparse result (for sparse S) whether i and j are scalars or vectors. Submatrix indexing on the left, as in A(i,j) = S, does not change the storage class of the matrix being modi ed. These decisions gave us some di culty. Cases like ~S and S >= T, where the result has many ones when the operands are sparse, made us consider adding more exceptions to the rules. We discussed the possibility of \sparse" matrices in which all the values not explicitly stored would be some scalar (like 1) rather than zero. We rejected these ideas in the interest of simplicity. 3. Implementation. This section describes the algorithms for the sparse operations in Matlab in some detail. We begin with a discussion of fundamental data structures and design decisions. of a data structure. The internal representation of a sparse matrix must be exible enough to implement all the Matlab operations. For simplicity, we ruled out the use of di erent data structures for di erent operations. The data structure should be compact, storing only nonzero elements, with a minimum of overhead storage for integers or pointers. Wherever possible, it should support matrix operations in time proportional to ops. Since Matlab is an interpreted, high-level matrix language, e ciency is more important in matrix arithmetic and matrix-vector operations than in accessing single elements of matrices. These goals are met by a simple column-oriented scheme that has been widely used in sparse matrix computation. A sparse matrix is a C record structure with the following constituents. The nonzero elements are stored in a one-dimensional array of double-precision reals, in column major order. (If the matrix is complex, the imaginary parts are stored in another such array.) A second array of integers stores the row indices. A third array of n + 1 integers stores the index into the rst two arrays of the leading entry in each of the n columns, and a terminating index whose value is nnz. Thus a real m n sparse matrix with nnz nonzeros uses nnz reals and nnz + n + 1 integers. This scheme is not e cient for manipulating matrices one element at a time: access to a single element takes time at least proportional to the logarithm of the length of its column inserting or removing a nonzero may require extensive data movement. However, element-by-element manipulation is rare in Matlab (and is expensive even in full Matlab). Its most common application would be to create a sparse matrix, but this is more e ciently done by building a list i j s] of matrix elements in arbitrary order and then using sparse(i,j,s) to create the matrix. The sparse data structure is allowed to have unused elements after the end of the last column of the matrix. Thus an algorithm that builds up a matrix one column at a time can be implemented e ciently by allocating enough space for all the expected nonzeros at the outset. 3.1.2. Storage allocation. Storage allocation is one of the thorniest parts of building portable systems. Matlab handles storage allocation for the user, invisibly allocating and deallocating storage as matrices appear, disappear, and change size.
8
3.1. Fundamentals. 3.1.1. Data structure. A most important implementationdecision is the choice
Sometimes the user can gain e ciency by preallocating storage for the result of a computation. One does this in full Matlab by allocating a matrix of zeros and lling it in incrementally. Similarly, in sparse Matlab one can preallocate a matrix (using sparse) with room for a speci ed number of nonzeros. Filling in the sparse matrix a column at a time requires no copying or reallocation. Within Matlab, simple \allocate" and \free" procedures handle storage allocation. (We will not discuss how Matlab handles its free storage and interfaces to the operating system to provide these procedures.) There is no provision for doing storage allocation within a single matrix a matrix is allocated as a single block of storage, and if it must expand beyond that block it is copied into a newly allocated larger block. Matlab must allocate space to hold the results of operations. For a full result, Matlab allocates mn elements at the start of the computation. This strategy could be disastrous for sparse matrices. Thus, sparse Matlab attempts to make a reasonable choice of how much space to allocate for a sparse result. Some sparse matrix operations, like Cholesky factorization, can predict in advance the exact amount of storage the result will require. These operations simply allocate a block of the right size before the computation begins. Other operations, like matrix multiplication and LU factorization, have results of unpredictable size. These operations are all implemented by algorithms that compute one column at a time. Such an algorithm rst makes a guess at the size of the result. If more space is needed at some point, it allocates a new block that is larger by a constant factor (typically 1.5) than the current block, copies the columns already computed into the new block, and frees the old block. Most of the other operations compute a simple upper bound on the storage required by the result to decide how much space to allocate|for example, the pointwise product S .* T uses the smaller of nnz(S) and nnz(T ), and S + T uses the smaller of nnz(S) + nnz(T) and mn. 3.1.3. The sparse accumulator. Many sparse matrix algorithms use a dense working vector to allow random access to the currently \active" column or row of a matrix. The sparse Matlab implementation formalizes this idea by de ning an abstract data type called the sparse accumulator, or spa. The spa consists of a dense vector of real (or complex) values, a dense vector of true/false \occupied" ags, and an unordered list of the indices whose occupied ags are true. The spa represents a column vector whose \unoccupied" positions are zero and whose \occupied" positions have values (zero or nonzero) speci ed by the dense real or complex vector. It allows random access to a single element in constant time, as well as sequencing through the occupied positions in constant time per element. Most matrix operations allocate the spa (with appropriate dimension) at their beginning and free it at their end. Allocating the spa takes time proportional to its dimension (to turn o all the occupied ags), but subsequent operations take only constant time per nonzero. In a sense the spa is a register and an instruction set in an abstract machine architecture for sparse matrix computation. Matlab manipulates the spa through some thirty-odd access procedures. About half of these are operations between the spa and a sparse or dense vector, from a \spaxpy" that implements spa := spa+ax (where a is a scalar and x is a column of a sparse matrix) to a \spaeq" that tests elementwise equality. Other routines load and store the spa, permute it, and access individual elements. The most complicated spa operation is a depth- rst search on
9
an acyclic graph, which marks as \occupied" a topologically ordered list of reachable vertices this is used in the sparse triangular solve described in Section 3.4.2. The spa simpli es data structure manipulation, because all ll occurs in the spa that is, only in the spa can a zero become nonzero. The \spastore" routine does not store exact zeros, and in fact the sparse matrix data structure never contains any explicit zeros. Almost all real arithmetic operations occur in spa routines, too, which simpli es Matlab's tally of ops. (The main exceptions are in certain scalar-matrix operations like 2*A, which are implemented without the spa for e ciency.) 3.1.4. Asymptotic complexity analysis. A strong philosophical principle in the sparse Matlab implementation is that it should be possible to analyze the complexity of the various operations, and that they should be e cient in the asymptotic sense as well as in practice. This section discusses this principle, in terms of both theoretical ideals and engineering compromises. Ideally all the matrix operations would use time proportional to ops, that is, their running time would be proportional to the number of nonzero real arithmetic operations performed. This goal cannot always be met: for example, 0 1] + 1 0] does no nonzero arithmetic. A more accurate statement is that time should be proportional to ops or data size, whichever is larger. Here \data size" means the size of the output and that part of the input that is used nontrivially for example, in A*b only those columns of A corresponding to nonzeros in b participate nontrivially. This more accurate ideal can be realized in almost all of Matlab. The exceptions are some operations that do no arithmetic and cannot be implemented in time proportional to data size. The algorithms to compute most of the reordering permutations described in Section 3.3 are e cient in practice but not linear in the worst case. Submatrix indexing is another example: if i and j are vectors of row and column indices, B = A(i,j) may examine all the nonzeros in the columns A(: j), and B(i,j) = A can at worst take time linear in the total size of B. The Matlab implementation actually violates the \time proportional to ops" philosophy in one systematic way. The list of occupied row indices in the spa is not maintained in numerical order, but the sparse matrix data structure does require row indices to be ordered. Sorting the row indices when storing the spa would theoretically imply an extra factor of O(log n) in the worst-case running times of many of the matrix operations. All our algorithms could avoid this factor|usually by storing the matrix with unordered row indices, then using a linear-time transposition sort to reorder all the rows of the nal result at once|but for simplicity of programming we included the sort in \spastore". The idea that running time should be susceptible to analysis helps the user who writes programs in Matlab to choose among alternative algorithms, gives guidance in scaling up running times from small examples to larger problems, and, in a generalpurpose system like Matlab, gives some insurance against an unexpected worst-case instance arising in practice. Of course complete a priori analysis is impossible| the work in sparse LU factorization depends on numerical pivoting choices, and the e cacy of a heuristic reordering such as minimum degree is unpredictable|but we feel it is worthwhile to stay as close to the principle as we can. In a technical report 14] we present some experimental evidence that sparse Matlab operations require time proportional to ops and data size in practice. 3.2. Factorizations. The LU and Cholesky factorizations of a sparse matrix yield sparse results. Matlab does not yet have a sparse QR factorization. Section 3.6 includes some remarks on sparse eigenvalue computation in Matlab.
10
3.2.1. LU Factorization. If A is a sparse matrix, L,U,P] = lu(A) returns three sparse matrices such that P A = LU, as obtained by Gaussian elimination with partial pivoting. The permutation matrix P uses only O(n) storage in sparse format. As in dense Matlab, L,U] = lu(A) returns a permuted unit lower triangular and an upper triangular matrix whose product is A. Since sparse LU must behave like Matlab's full LU, it does not pivot for sparsity. A user who happens to know a good column permutation Q for sparsity can, of course, ask for lu(A*Q'), or lu(A(:,q)) where q is an integer permutation vector. Section 3.3 describes a few ways to nd such a permutation. The matrix division operators \ and / do pivot for sparsity by default see Section 3.4. We use a version of the GPLU algorithm 15] to compute the LU factorization. This computes one column of L and U at a time by solving a sparse triangular system with the already- nished columns of L. Section 3.4.2 describes the sparse triangular solver that does most of the work. The total time for the factorization is proportional to the number of nonzero arithmetic operations (plus the size of the result), as desired. The column-oriented data structure for the factors is created as the factorization progresses, never using any more storage for a column than it requires. However, the total size of L or U cannot be predicted in advance. Thus the factorization routine makes an initial guess at the required storage, and expands that storage (by a factor of 1:5) whenever necessary. 3.2.2. Cholesky factorization. As in full Matlab, R = chol(A) returns the upper triangular Cholesky factor of a Hermitian positive de nite matrix A. Pivoting for sparsity is not automatic, but minimum degree and pro le-limiting permutations can be computed as described in Section 3.3. Our current implementation of Cholesky factorization emphasizes simplicity and compatibility with the rest of sparse Matlab thus it does not use some of the more sophisticated techniques such as the compressed index storage scheme 11, Sec. 5.4.2], or supernodal methods to take advantage of the clique structure of the chordal graph of the factor 2]. It does, however, run in time proportional to arithmetic operations with little overhead for data structure manipulation. We use a slightly simpli ed version of an algorithm from the Yale Sparse Matrix Package 9], which is described in detail by George and Liu 11]. We begin with a combinatorial step that determines the number of nonzeros in the Cholesky factor (assuming no exact cancellation) and allocates a large enough block of storage. We then compute the lower triangular factor RT one column at a time. Unlike YSMP and Sparspak, we do not begin with a symbolic factorization instead, we create the sparse data structure column by column as we compute the factor. The only reason for the initial combinatorial step is to determine how much storage to allocate for the result. 3.3. Permutations. A permutation of the rows or columns of a sparse matrix A can be represented in two ways. A permutation matrix P acts on the rows of A as P*A or on the columns as A*P'. A permutation vector p, which is a full vector of length n containing a permutation of 1:n, acts on the rows of A as A(p,:) or on the columns as A(:,p). Here p could be either a row vector or a column vector. Both representations use O(n) storage, and both can be applied to A in time proportional to nnz(A). The vector representation is slightly more compact and e cient, so the various sparse matrix permutation routines all return vectors|full row vectors, to be precise|with the exception of the pivoting permutation in LU factorization.
11
Converting between the representations is almost never necessary, but it is simple. If I is a sparse identity matrix of the appropriate size, then P is I(p,:) and P T is I(:,p). Also p is (P*(1:n)')' or (1:n)*P'. (We leave to the reader the puzzle of using find to obtain p from P without doing any arithmetic.) The inverse of P is P' the inverse r of p can be computed by the \vectorized" statement r(p) = 1:n. 3.3.1. Permutations for sparsity: Asymmetric matrices. Reordering the columns of a matrix can often make its LU or QR factors sparser. The simplest such reordering is to sort the columns by increasing nonzero count. This is sometimes a good reordering for matrices with very irregular structures, especially if there is great variation in the nonzero counts of rows or columns. The Matlab function p = colperm(A) computes this column-count permutation. It is implemented as a two-line m- le:
i,j] = find(A) ignore,p] = sort(diff(find(diff( 0 j' inf]))))
The vector j is the column indices of all the nonzeros in A, in column major order. The inner diff computes rst di erences of j to give a vector with ones at the starts of columns and zeros elsewhere the find converts this to a vector of column-start indices the outer diff gives the vector of column lengths and the second output argument from sort is the permutation that sorts this vector. The symmetric reverse Cuthill-McKee ordering described in Section 3.3.2 can be used for asymmetric matrices as well the function symrcm(A) actually operates on the nonzero structure of A+AT . This is sometimes a good ordering for matrices that come from one-dimensional problems or problems that are in some sense long and thin. Minimum degree is an ordering that often performs better than colperm or symrcm. The sparse Matlab function p = colmmd(A) computes a minimum degree ordering for the columns of A. This column ordering is the same as a symmetric minimum degree ordering for the matrix AT A, though we do not actually form AT A to compute it. George and Liu 10] survey the extensive development of e cient and e ective versions of symmetric minimum degree, most of which is re ected in the symmetric minimum degree codes in Sparspak, YSMP, and the Harwell Subroutine Library. The Matlab version of minimum degree uses many of these ideas, as well as some ideas from a parallel symmetric minimum degree algorithm by Gilbert, Lewis, and Schreiber 13]. We sketch the algorithm brie y to show how these ideas are expressed in the framework of column minimum degree. The reader who is not interested in all the details can skip to Section 3.3.2. Although most column minimum degree codes for asymmetric matrices are based on a symmetric minimum degree code, our organization is the other way around: Matlab's symmetric minimum degree code (described in Section 3.3.2) is based on its column minimum degree code. This is because the best way to represent a symmetric matrix (for the purposes of minimum degree) is as a union of cliques, or full submatrices. When we begin with an asymmetric matrix A, we wish to reorder its columns by using a minimum degree order on the symmetric matrix AT A|but each row of A induces a clique in AT A, so we can simply use A itself to represent AT A instead of forming the product explictly. Speelpenning 24] called such a clique representation of a symmetric graph the \generalized element" representation George and
12
Liu 10] call it the \quotient graph model." Ours is the rst column minimum degree implementation that we know of whose data structures are based directly on A, and which does not need to spend the time and storage to form the structure of AT A. The idea for such a code is not new, however|George and Liu 10] suggest it, and our implementation owes a great deal to discussions between the rst author and Esmond Ng and Barry Peyton of Oak Ridge National Laboratories. We simulate symmetric Gaussian elimination on AT A, using a data structure that represents A as a set of vertices and a set of cliques whose union is the graph of AT A. Initially, each column of A is a vertex and each row is a clique. Elimination of a vertex j induces ll among all the (so far uneliminated) vertices adjacent to j. This means that all the vertices in cliques containing j become adjacent to one another. Thus all the cliques containing vertex j merge into one clique. In other words, all the rows of A with nonzeros in column j disappear, to be replaced by a single row whose nonzero structure is their union. Even though ll is implicitly being added to AT A, the data structure for A gets smaller as the rows merge, so no extra storage is required during the elimination. Minimum degree chooses a vertex of lowest degree (the sparsest remaining column of AT A, or the column of A having nonzero rows in common with the fewest other columns), eliminates that vertex, and updates the remainder of A by adding ll (i.e. merging rows). This whole process is called a \stage" after n stages the columns are all eliminated and the permutation is complete. In practice, updating the data structure after each elimination is too slow, so several devices are used to perform many eliminations in a single stage before doing the update for the stage. First, instead of nding a single minimum-degree vertex, we nd an entire \independent set" of minimum-degree vertices with no common nonzero rows. Eliminating one such vertex has no e ect on the others, so we can eliminate them all at the same stage and do a single update. George and Liu call this strategy \multiple elimination". (They point out that the resulting permutation may not be a strict minimum degree order, but the di erence is generally insigni cant.) Second, we use what George and Liu call \mass elimination": After a vertex j is eliminated, its neighbors in AT A form a clique (a single row in A). Any of those neighbors whose own neighbors all lie within that same clique will be a candidate for elimination at the next stage. Thus, we may as well eliminate such a neighbor during the same stage as j, immediately after j, delaying the update until afterward. This often saves a tremendous number of stages because of the large cliques that form late in the elimination. (The number of stages is reduced from the height of the elimination tree to approximately the height of the clique tree for many two-dimensional nite element problems, for example, this reduces the number of stages from about pn to about log n.) Mass elimination is particularly simple to implement in the column data structure: after all rows with nonzeros in column j are merged into one row, the columns to be eliminated with j are those whose only remaining nonzero is in that new row. Third, we note that any two columns with the same nonzero structure will be eliminated in the same stage by mass elimination. Thus we allow the option of combining such columns into \supernodes" (or, as George and Liu call them, \indistinguishable nodes"). This speeds up the ordering by making the data structure for A smaller. The degree computation must account for the sizes of supernodes, but this turns out to be an advantage for two reasons. The quality of the ordering actually improves slightly if the degree computation does not count neighbors within the same
13
supernode. (George and Liu observe this phenomenon and call the number of neighbors outside a vertex's supernode its \external degree.") Also, supernodes improve the approximate degree computation described below. Amalgamating columns into supernodes is fairly slow (though it takes time only proportional to the size of A). Supernodes can be amalgamated at every stage, periodically, or never the current default is every third stage. Fourth, we note that the structure of AT A is not changed by dropping any row of A whose nonzero structure is a subset of that of another row. This row reduction speeds up the ordering by making the data structure smaller. More signi cantly, it allows mass elimination to recognize larger cliques, which decreases the number of stages dramatically. Du and Reid 8] call this strategy \element absorption." Row reduction takes time proportional to multiplying AAT in the worst case (though the worst case is rarely realized and the constant of proportionality is very small). By default, we reduce at every third stage again the user can change this. Fifth, to achieve larger independent sets and hence fewer stages, we relax the minimum degree requirement and allow elimination of any vertex of degree at most d+ , where d is the minimum degree at this stage and and are parameters. The choice of threshold can be used to trade o ordering time for quality of the resulting ordering. For problems that are very large, have many right-hand sides, or factor many matrices with the same nonzero structure, ordering time is insigni cant and the tightest threshold is appropriate. For one-o problems of moderate size, looser thresholds like 1:5d+2 or even 2d+10 may be appropriate. The threshold can be set by the user its default is 1:2d + 1. Sixth and last, our code has the option of using an \approximate degree" instead of computing the actual vertex degrees. Recall that a vertex is a column of A, and its degree is the number of other columns with which it shares some nonzero row. Computing all the vertex degrees in AT A takes time proportional to actually computing AT A, though the constant is quite small and no extra space is needed. Still, the exact degree computation can be the slowest part of a stage. If column j is a supernode containing n(j) original columns, we de ne its approximate degree as X d(j) = (nnz(A(i :)) ; n(j)):
aij =0
6
This can be interpreted as the sum of the sizes of the cliques containing j, except that j and the other columns in its supernode are not counted. This is a fairly good approximation in practice it errs only by overcounting vertices that are members of at least three cliques containing j. George and Liu call such vertices \outmatched nodes," and observe that they tend to be rare in the symmetric algorithm. Computing approximate degrees takes only time proportional to the size of A. Column minimum degree sometimes performs poorly if the matrix A has a few very dense rows, because then the structure of AT A consists mostly of the cliques induced by those rows. Thus colmmd will withhold from consideration any row containing more than a xed proportion (by default, 50%) of nonzeros. All these options for minimum degree are under the user's control, though the casual user of Matlab never needs to change the defaults. The default settings use approximate degrees, row reduction and supernode amalgamation every third stage, and a degree threshold of 1:2d + 1, and withhold rows that are at least 50% dense. 3.3.2. Permutations for sparsity: Symmetric matrices. Preorderings for Cholesky factorization apply symmetrically to the rows and columns of a symmetric
14
positive de nite matrix. Sparse Matlab includes two symmetric preordering permutation functions. The colperm permutation can also be used as a symmetric ordering, but it is usually not the best choice. Bandwidth-limiting and pro le-limiting orderings are useful for matrices whose structure is \one-dimensional" in a sense that is hard to make precise. The reverse Cuthill-McKee ordering is an e ective and inexpensive pro le-limiting permutation. Matlab function p = symrcm(A) returns a reverse Cuthill-McKee permutation for symmetric matrix A. The algorithm rst nds a \pseudo-peripheral" vertex of the graph of A, then generates a level structure by breadth- rst search and orders the vertices by decreasing distance from the pseudo-peripheral vertex. Our implementation is based closely on the Sparspak implementation as described in the book by George and Liu 11]. Pro le methods like reverse Cuthill-McKee are not the best choice for most large matrices arising from problems with two or more dimensions, or problems without much geometric structure, because such matrices typically do not have reorderings with low pro le. The most generally useful symmetric preordering in Matlab is minimum degree, obtained by the function p = symmmd(A). Our symmetric minimum degree implementation is based on the column minimum degree described in Section 3.3.1. In fact, symmmd just creates a nonzero structure K with a column for each column of A and a row for each above-diagonal nonzero in A, such that K T K has the same nonzero structure as A it then calls the column minimum degree code on K. 3.3.3. Nonzero diagonals and block triangular form. A square nonsingular matrix A always has a row permutation p such that A(p :) has nonzeros on its main diagonal. The Matlab function p = dmperm(A) computes such a permutation. With two output arguments, the function p,q] = dmperm(A) gives both row and column permutations that put A into block upper triangular form that is, A(p q) has a nonzero main diagonal and a block triangular structure with the largest possible number of blocks. Notice that the permutations p returned by these two calls are likely to be di erent. The most common application of block triangular form is to solve a reducible system of linear equations by block back substitution, factoring only the diagonal blocks of the matrix. Figure 9 is an m- le that implements this algorithm. The m- le illustrates the call p,q,r] = dmperm(A), which returns p and q as before, and also a vector r giving the boundaries of the blocks of the block upper triangular form. To be precise, if there are b blocks in each direction, then r has length b +1, and the i-th diagonal block of A(p q) consists of rows and columns with indices from r(i) through r(i + 1) ; 1. Any matrix, whether square or not, has a form called the \Dulmage-Mendelsohn decomposition" 4, 20], which is the same as ordinary block upper triangular form if the matrix is square and nonsingular. The most general form of the decomposition, for arbitrary rectangular A, is p,q,r,s] = dmperm(A). The rst two outputs are permutations that put A(p q) into block form. Then r describes the row boundaries of the blocks and s the column boundaries: the i-th diagonal block of A(p q) has rows r(i) through r(i + 1) ; 1 and columns s(i) through s(i + 1) ; 1. The rst diagonal block may have more columns than rows, the last diagonal block may have more rows than columns, and all the other diagonal blocks are square. The subdiagonal blocks are all zero. The square diagonal blocks have nonzero diagonal elements. All the diagonal blocks are irreducible for the non-square blocks, this means that they have the \strong Hall property" 4]. This block form can be used to solve least squares
15
