Fill-in reducing, Leverage fast dense algebra
Requires the SuperLU library, (BSD-like)
Here SPD means symmetric positive definite.
All these solvers follow the same general concept. Here is a typical and general example:
#include <Eigen/RequiredModuleName> // ... SparseMatrix<double> A; // fill A VectorXd b, x; // fill b // solve Ax = b SolverClassName<SparseMatrix<double> > solver; solver.compute(A); if(solver.info()!=Succeeded) { // decomposition failed return; } x = solver.solve(b); if(solver.info()!=Succeeded) { // solving failed return; } // solve for another right hand side: x1 = solver.solve(b1);
For SPD solvers, a second optional template argument allows to specify which triangular part have to be used, e.g.:
#include < Eigen/IterativeLinearSolvers > ConjugateGradient<SparseMatrix<double>, Eigen::Upper > solver; x = solver.compute(A).solve(b);
In the above example, only the upper triangular part of the input matrix A is considered for solving. The opposite triangle might either be empty or contain arbitrary values.
In the case where multiple problems with the same sparcity pattern have to be solved, then the "compute" step can be decomposed as follow:
SolverClassName<SparseMatrix<double> > solver; solver.analyzePattern(A); // for this step the numerical values of A are not used solver.factorize(A); x1 = solver.solve(b1); x2 = solver.solve(b2); ... A = ...; // modify the values of the nonzeros of A, the nonzeros pattern must stay unchanged solver.factorize(A); x1 = solver.solve(b1); x2 = solver.solve(b2); ...
The compute() method is equivalent to calling both analyzePattern() and factorize().
Finally, each solver provides some specific features, such as determinant, access to the factors, controls of the iterations, and so on. More details are availble in the documentations of the respective classes.
Supported operators and functions
Because of their special storage format, sparse matrices cannot offer the same level of flexbility than dense matrices. In Eigen's sparse module we chose to expose only the subset of the dense matrix API which can be efficiently implemented. In the following sm denotes a sparse matrix, sv a sparse vector, dm a dense matrix, and dv a dense vector.
Basic operations
Sparse expressions support most of the unary and binary coefficient wise operations:
sm1.real()   sm1.imag()   -sm1                    0.5*sm1 sm1+sm2      sm1-sm2      sm1.cwiseProduct(sm2)
However, a strong restriction is that the storage orders must match. For instance, in the following example:
sm4 = sm1 + sm2 + sm3;
sm1, sm2, and sm3 must all be row-major or all column major. On the other hand, there is no restriction on the target matrix sm4. For instance, this means that for computing
, the matrix
must be evaluated into a temporary matrix of compatible storage order:
SparseMatrix<double> A, B; B = SparseMatrix<double>(A.transpose()) + A;
Binary coefficient wise operators can also mix sparse and dense expressions:
sm2 = sm1.cwiseProduct(dm1); dm2 = sm1 + dm1;
Sparse expressions also support transposition:
sm1 = sm2.transpose(); sm1 = sm2.adjoint();
However, there is no transposeInPlace() method.
Matrix products
Eigen supports various kind of sparse matrix products which are summarize below:
sparse-dense:
dv2 = sm1 * dv1; dm2 = dm1 * sm1.adjoint(); dm2 = 2. * sm1 * dm1;
symmetric sparse-dense. The product of a sparse symmetric matrix with a dense matrix (or vector) can also be optimized by specifying the symmetry with selfadjointView():
dm2 = sm1.selfadjointView<>() * dm1; // if all coefficients of A are stored dm2 = A.selfadjointView< Upper >() * dm1; // if only the upper part of A is stored dm2 = A.selfadjointView< Lower >() * dm1; // if only the lower part of A is stored
sparse-sparse. For sparse-sparse products, two different algorithms are available. The default one is conservative and preserve the explicit zeros that might appear:
sm3 = sm1 * sm2; sm3 = 4 * sm1.adjoint() * sm2;
The second algorithm prunes on the fly the explicit zeros, or the values smaller than a given threshold. It is enabled and controlled through the prune() functions:
sm3 = (sm1 * sm2).prune(); // removes numerical zeros sm3 = (sm1 * sm2).prune(ref); // removes elements much smaller than ref sm3 = (sm1 * sm2).prune(ref,epsilon); // removes elements smaller than ref*epsilon
permutations. Finally, permutations can be applied to sparse matrices too:
PermutationMatrix<Dynamic,Dynamic> P = ...; sm2 = P * sm1; sm2 = sm1 * P.inverse(); sm2 = sm1.transpose() * P;
Triangular and selfadjoint views
Just as with dense matrices, the triangularView() function can be used to address a triangular part of the matrix, and perform triangular solves with a dense right hand side:
dm2 = sm1.triangularView< Lower >(dm1); dv2 = sm1.transpose().triangularView< Upper >(dv1);
The selfadjointView() function permits various operations:
optimized sparse-dense matrix products:
dm2 = sm1.selfadjointView<>() * dm1; // if all coefficients of A are stored dm2 = A.selfadjointView< Upper >() * dm1; // if only the upper part of A is stored dm2 = A.selfadjointView< Lower >() * dm1; // if only the lower part of A is stored
copy of triangular parts:
sm2 = sm1.selfadjointView< Upper >(); // makes a full selfadjoint matrix from the upper triangular part sm2.selfadjointView< Lower >() = sm1.selfadjointView< Upper >(); // copies the upper triangular part to the lower triangular part
application of symmetric permutations:
PermutationMatrix<Dynamic,Dynamic> P = ...; sm2 = A.selfadjointView< Upper >().twistedBy(P); // compute P S P' from the upper triangular part of A, and make it a full matrix sm2.selfadjointView< Lower >() = A.selfadjointView< Lower >().twistedBy(P); // compute P S P' from the lower triangular part of A, and then only compute the lower part
Sub-matrices
Sparse matrices does not support yet the addressing of arbitrary sub matrices. Currently, one can only reference a set of contiguous inner vectors, i.e., a set of contiguous rows for a row-major matrix, or a set of contiguous columns for a column major matrix:
sm1.innerVector(j); // returns an expression of the j-th column (resp. row) of the matrix if sm1 is col-major (resp. row-major) sm1.innerVectors(j, nb); // returns an expression of the nb columns (resp. row) starting from the j-th column (resp. row) // of the matrix if sm1 is col-major (resp. row-major) sm1.middleRows(j, nb); // for row major matrices only, get a range of nb rows sm1.middleCols(j, nb); // for column major matrices only, get a range of nb columns
