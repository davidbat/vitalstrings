Publication date: 2009-02-26
Patent application number: 20090055615
Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP
Abstract:
A method, system and computer program product for garbage collection      sensitive load balancing is disclosed. The method for memory tuning for      garbage collection and CPU utilization optimization can include      benchmarking an application across multiple different heap sizes to      accumulate garbage collection metrics and utilizing the garbage      collection metrics accumulated during benchmarking to compute both CPU      utilization and garbage collection time for each of a selection of      candidate heap sizes. One of the candidate heap sizes can be matched to a      desired CPU utilization and garbage collection time, and the matched one      of the candidate heap sizes can be applied to a host environment.
Claims:
1. A method for memory tuning for garbage collection and central      processing unit (CPU) utilization optimization, the method      comprising:benchmarking an application across multiple different heap      sizes to accumulate garbage collection metrics;utilizing the garbage      collection metrics accumulated during benchmarking to compute both CPU      utilization and garbage collection time for each of a selection of      candidate heap sizes;matching one of the candidate heap sizes to a      desired CPU utilization and garbage collection time; and,applying the      matched one of the candidate heap sizes to a host environment.
2. The method of claim 1, wherein benchmarking an application across      multiple different heap sizes, comprises benchmarking an application      across multiple different heap sizes to determine a rate of garbage      collections, an amount of memory collected for each garbage collection      activity, and an average duration of each garbage collection.
3. The method of claim 2, wherein utilizing the garbage collection metrics      accumulated during benchmarking to compute both CPU utilization and      garbage collection time for each of a selection of candidate heap sizes,      comprises:computing CPU utilization for a candidate heap size as the      number of CPU seconds used for a garbage collection divided by a number      of threads involved in the garbage collection; and,further computing a      total garbage collection time for the candidate heap size as a base      garbage collection time divided by a number of threads involved in the      garbage collection combined with an average sweep time for the garbage      collection.
4. (canceled)
5. The method of claim 1, wherein utilizing the garbage collection metrics      accumulated during benchmarking to compute both CPU utilization and      garbage collection time for each of a selection of candidate heap sizes,      further comprises calculating a number of threads for use in garbage      collection to meet stated quality of service (QoS) goals.
6. The method of claim 1, wherein utilizing the garbage collection metrics      accumulated during benchmarking to compute both CPU utilization and      garbage collection time for each of a selection of candidate heap sizes,      further comprises calculating a number of CPUs to be provisioned to meet      stated quality of service (QoS) goals.
7. The method of claim 1, further comprising adding additional processors      when a match of the candidate heap sizes to a desired CPU utilization and      garbage collection time cannot be found.
8. A garbage collection data processing system comprising:a host      environment configured for garbage collection;a heap of particular heap      size coupled to the host environment and configured for use by      applications executing in the host environment; and,a host environment      tuner coupled to the host environment, the tuner comprising program code      enabled to benchmark an application across multiple different heap sizes      of the heap to accumulate garbage collection metrics, utilize the garbage      collection metrics accumulated during benchmarking to compute both CPU      utilization and garbage collection time for each of a selection of      candidate heap sizes, match one of the candidate heap sizes to a desired      CPU utilization and garbage collection time, and apply the matched one of      the candidate heap sizes to the host environment.
9. The data processing system of claim 8, wherein the host environment is      a virtual machine.
10. A computer-readable storage medium having stored therein computer      instructions for memory tuning for garbage collection and central      processing unit (CPU) utilization optimization, the computer instructions      which, when executed by a computer system, cause the computer system to      perform operations comprising:benchmarking an application across multiple      different heap sizes to accumulate garbage collection metrics;utilizing      the garbage collection metrics accumulated during benchmarking to compute      both CPU utilization and garbage collection time for each of a selection      of candidate heap sizes;matching one of the candidate heap sizes to a      desired CPU utilization and garbage collection time; and,applying the      matched one of the candidate heap sizes to a host environment
11. The computer-readable storage medium of claim 10, wherein the      benchmarking an application across multiple different heap sizes,      comprises benchmarking an application across multiple different heap      sizes to determine a rate of garbage collections, an amount of memory      collected for each garbage collection activity, and an average duration      of each garbage collection.
12. The computer-readable storage medium of claim 11, wherein the      utilizing the garbage collection metrics accumulated during benchmarking      to compute both CPU utilization and garbage collection time for each of a      selection of candidate heap sizes, comprises:computing CPU utilization      for a candidate heap size as the number of CPU seconds used for a garbage      collection divided by a number of threads involved in the garbage      collection; and,further computing a total garbage collection time for the      candidate heap size as a base garbage collection time divided by a number      of threads involved in the garbage collection combined with an average      sweep time for the garbage collection.
13. (canceled)
14. The computer-readable storage medium of claim 10, wherein the      utilizing the garbage collection metrics accumulated during benchmarking      to compute both CPU utilization and garbage collection time for each of a      selection of candidate heap sizes, further comprises calculating a number      of threads for use in garbage collection to meet stated quality of      service (QoS) goals.
15. The computer-readable storage medium of claim 10, wherein the      utilizing the garbage collection metrics accumulated during benchmarking      to compute both CPU utilization and garbage collection time for each of a      selection of candidate heap sizes, further comprises calculating a number      of CPUs to be provisioned to meet stated quality of service (QoS) goals.
16. The computer-readable storage medium of claim 10, further comprising      adding additional processors when a match of the candidate heap sizes to      a desired CPU utilization and garbage collection time cannot be found.
Description:
CROSS-REFERENCE TO RELATED APPLICATIONS
[0001]This application is a Divisional of U.S. application Ser. No.      11/382,161, filed May 8, 2006, entitled "MEMORY TUNING FOR GARBAGE      COLLECTION AND CENTRAL PROCESSING UNIT (CPU) UTILIZATION OPTIMIZATION,"      which is incorporated herein by reference in its entirety.
BACKGROUND OF THE INVENTION
[0002]1. Field of the Invention
[0003]The present invention relates to the field of memory management and      more particularly to the field of garbage collection for memory      management.
[0004]2. Description of the Related Art
[0005]Memory leakage has confounded software developers for decades      resulting in the sometimes global distribution of bug-ridden, crash-prone      software applications. Particularly in respect to those programming      languages which permitted the manual allocation of memory, but also      required the manual de-allocation of allocated memory, memory leakage has      proven to be the principal run-time bug most addressed during the      software development cycle. So prevalent a problem has memory leakage      become, entire software development tools have been developed and      marketed solely to address the memory leakage problem.
[0006]Memory leakage, broadly defined, is the gradual loss of allocable      memory due to the failure to de-allocate previously allocated, but no      longer utilized memory. Typically, memory can be reserved for data having      a brief lifespan. Once the lifespan has completed, the reserved memory      ought to be returned to the pool of allocable memory so that the reserved      memory can be used at a subsequent time as necessary. Importantly, where      memory leakage persists without remediation, ultimately not enough memory      will remain to accommodate the needs of other processes.
[0007]Recognizing the importance of addressing the memory leakage problem,      computer programming language theorists have developed the notion of      garbage collection. Garbage collection refers to the automated analysis      of allocated memory to identify regions of allocated memory containing      data which no longer are required for the operation of associated      processes. In the context of object oriented programming languages such      as the Java.Â®. programming language, when objects residing in memory      are no longer accessible within a corresponding application, the memory      allocated to the "dead" object can be returned to the pool of allocable      memory.
[0008]The process of garbage collection can be time consuming and can      result in a degradation of performance for a hosted application. A      primary factor affecting the time consumption of a garbage collection      operation can include heap size. Generally, the larger the heap size, the      more time consuming a garbage collection operation can be. Heap size,      however, can be limited for a virtual machine for a number of reasons      unrelated to garbage collection. To circumvent the limitation on heap      size, it is common to utilize multiple virtual machines for a single      central processing unit (CPU) in order to support the execution of a      hosted application. Notwithstanding, the typical garbage collection      operation can fully utilize a supporting CPU such that a garbage      collection operation in one virtual machine can degrade the performance      of another virtual machine supported by the same CPU.
[0009]In most cases, the degradation of performance will have little      impact on the performance of a hosted application as most hosted      applications are not time sensitive. However, some classes of hosted      applications, including soft real-time systems, depend upon consistent      performance at a guaranteed level of Quality of Service (QoS). Generally,      soft real-time systems include speech recognition and text to speech      systems. As it will be well understood in the art, soft real-time systems      prefer to avoid the degradation in performance caused by garbage      collection.
BRIEF SUMMARY OF THE INVENTION
[0010]Embodiments of the present invention address deficiencies of the art      in respect to load balancing in an enterprise environment and provide a      novel and non-obvious method, system and apparatus for garbage collection      sensitive load balancing. In a first embodiment of the invention, a      method for memory tuning for garbage collection and CPU utilization      optimization can be provided. The method can include benchmarking an      application across multiple different heap sizes to accumulate garbage      collection metrics and utilizing the garbage collection metrics      accumulated during benchmarking to compute both CPU utilization and      garbage collection time for each of a selection of candidate heap sizes.      One of the candidate heap sizes can be matched to a desired CPU      utilization and garbage collection time. As such, the matched one of the      candidate heap sizes can be applied to a host environment.
[0011]In a particular aspect of the embodiment, a maximum CPU utilization      can be determined that is acceptable for a QoS goal. In another aspect of      the embodiment, a desired garbage collection time can be determined as a      maximum garbage collection time consumed that is acceptable for a QoS      goal. In both circumstances, multiple virtual machines can share a host      platform without allowing the garbage collection process of each virtual      machine to invalidate the QoS requirements for each other virtual      machine.
[0012]In another embodiment of the invention, a garbage collection data      processing system can be provided. The system can include a host      environment, such as a virtual machine, configured for garbage      collection, a heap of particular heap size coupled to the host      environment and configured for use by applications executing in the host      environment, and a host environment tuner coupled to the host      environment. The tuner can include program code enabled to benchmark an      application across multiple different heap sizes of the heap to      accumulate garbage collection metrics, utilize the garbage collection      metrics accumulated during benchmarking to compute both CPU utilization      and garbage collection time for each of a selection of candidate heap      sizes, match one of the candidate heap sizes to a desired CPU utilization      and garbage collection time, and apply the matched one of the candidate      heap sizes to the host environment.
[0013]Additional aspects of the invention will be set forth in part in the      description which follows, and in part will be obvious from the      description, or may be learned by practice of the invention. The aspects      of the invention will be realized and attained by means of the elements      and combinations particularly pointed out in the appended claims. It is      to be understood that both the foregoing general description and the      following detailed description are exemplary and explanatory only and are      not restrictive of the invention, as claimed.
BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS
[0014]The accompanying drawings, which are incorporated in and constitute      part of this specification, illustrate embodiments of the invention and      together with the description, serve to explain the principles of the      invention. The embodiments illustrated herein are presently preferred, it      being understood, however, that the invention is not limited to the      precise arrangements and instrumentalities shown, wherein:
[0015]FIG. 1 is a schematic illustration of a memory tuning data      processing system enabled for garbage collection and CPU utilization      optimization; and,
[0016]FIG. 2 is a flow chart illustrating a process for memory tuning for      garbage collection and CPU utilization optimization.
DETAILED DESCRIPTION OF THE INVENTION
[0017]Embodiments of the present invention provide a method, system and      computer program product for memory tuning for garbage collection and CPU      utilization optimization. In accordance with an embodiment of the present      invention, an application can be benchmarked across multiple different      heap sizes to determine the rate of garbage collections, the amount of      memory collected for each garbage collection activity, and the average      duration of each garbage collection. Utilizing metrics accumulated during      benchmarking, both CPU utilization and garbage collection time can be      computed for each of a selection of candidate heap sizes. Subsequently, a      candidate heap size can be matched to a desired CPU utilization and      garbage collection time and applied to the host environment.
[0018]In illustration, FIG. 1 is a schematic illustration of a memory      tuning data processing system enabled for garbage collection time and CPU      utilization optimization. The data processing system can include a host      computing platform 100. The host computing platform 100 can include a CPU      110, current memory 120 and fixed storage 130. An operating system 140      can moderate the utilization of the CPU 110, current memory 120 and fixed      storage 130 for one or more host virtual machines 150. It is to be noted,      however, that the virtual machines 150 can directly moderate access to      the CPU 110, current memory 120 and fixed storage 130 in the absence of      an intermediate operating system 140. In any case, each virtual machine      150 can host the operation of one or more applications 160.
[0019]Each virtual machine 150 can be configured to maximally consume a      set amount of memory according to a pre-configured heap size. In further      illustration, a heap 170 can be allocated for use by the virtual machine      150. The selection of a heap size for the heap 170 can be applied by the      virtual machine tuner 200. In this regard, the virtual machine tuner 200      can include program code enabled to benchmark an application 160      operating in the virtual machine 150 across multiple different heap sizes      for the heap 170 utilizing timer/clock 190B. The benchmarking can produce      CPU utilization and garbage collection time metrics 190A for each heap      size. Consequently, utilizing the metrics 190A, a target CPU utilization      and garbage collection time 180 can be matched to a particular heap size      in order to select an optimal heap size for the heap 170. Further, each      virtual machine 150 can be configured to limit the number of threads      permitted to engage in garbage collection activities.
[0020]In more particular illustration of the operation of the virtual      machine tuner 200, FIG. 2 is a flow chart illustrating a process for      memory tuning for garbage collection and CPU utilization optimization.      Beginning in block 210, an application can be benchmarked across a number      of heap sizes for the virtual machine. In particular, the benchmarking      process can include measuring a rate of garbage collections, an amount of      memory collected in each garbage collection activity, and the average      duration of each garbage collection. In block 215, a first candidate heap      size can be selected for determining optimization. Additionally, QoS      input parameters can be provided, including maximum garbage collection      delay, maximum number of threads to be allocated for garbage collection      and a maximum CPU utilization permitted.
[0021]In block 220, a number of CPU seconds used for each garbage      collection activity for the candidate heap size can be computed by      measuring CPU utilization for the garbage collection activity and      multiplying the CPU utilization by the time consumed by the CPU in total      during that period. Concurrently, in block 225, an amount of time      consumed by a single thread performing the garbage collection activity      can be computed. As well, a base garbage collection time can be computed      in block 230. The base garbage collection time can include the minimal      amount of time required to mark and sweep threads during mark and sweep      style garbage collection.
[0022]In block 235, the CPU utilization for the candidate heap size can be      computed as the number of CPU seconds used for each garbage collection      divided by a number of threads involved in the garbage collection.      Likewise, in block 240, a total garbage collection time can be computed      for the candidate heap size as the base garbage collection time divided      by the number of threads involved in the garbage collection combined with      the average sweep time for the mark and sweep operation. Thereafter, in      block 245 the resulting CPU utilization and total garbage collection time      can be compared to predetermined performance objectives.
[0023]If a match is found in decision block 250, in block 255 the      candidate heap size can be established for the virtual machine.      Otherwise, in decision block 260 if additional candidate heap sizes      remain to be evaluated, in block 260 a next candidate heap size can be      selected for analysis. Subsequently, the matching process can repeat      through blocks 220 and 225 and 230. When no further candidate heap sizes      remain to be analyzed, and if no match has been found for the      pre-determined performance objectives in decision block 255, in block 270      a default heap size can be established for the virtual machine      irrespective of the pre-determined performance objectives. In this      circumstance, it can be recommended that additional processors are added      to the machine to achieve optimization. Also, a number CPUs required to      meet the pre-determined performance objectives, and a default number of      recommended threads for garbage collection to meet the performance      objectives can be recommended.
[0024]Embodiments of the invention can take the form of an entirely      hardware embodiment, an entirely software embodiment or an embodiment      containing both hardware and software elements. In a preferred      embodiment, the invention is implemented in software, which includes but      is not limited to firmware, resident software, microcode, and the like.      Furthermore, the invention can take the form of a computer program      product accessible from a computer-usable or computer-readable medium      providing program code for use by or in connection with a computer or any      instruction execution system.
[0025]For the purposes of this description, a computer-usable or computer      readable medium can be any apparatus that can contain, store,      communicate, propagate, or transport the program for use by or in      connection with the instruction execution system, apparatus, or device.      The medium can be an electronic, magnetic, optical, electromagnetic,      infrared, or semiconductor system (or apparatus or device) or a      propagation medium. Examples of a computer-readable medium include a      semiconductor or solid state memory, magnetic tape, a removable computer      diskette, a random access memory (RAM), a read-only memory (ROM), a rigid      magnetic disk and an optical disk. Current examples of optical disks      include compact disk-read only memory (CD-ROM), compact disk-read/write      (CD-R/W) and DVD.
[0026]A data processing system suitable for storing and/or executing      program code will include at least one processor coupled directly or      indirectly to memory elements through a system bus. The memory elements      can include local memory employed during actual execution of the program      code, bulk storage, and cache memories which provide temporary storage of      at least some program code in order to reduce the number of times code      must be retrieved from bulk storage during execution. Input/output or I/O      devices (including but not limited to keyboards, displays, pointing      devices, etc.) can be coupled to the system either directly or through      intervening I/O controllers. Network adapters may also be coupled to the      system to enable the data processing system to become coupled to other      data processing systems or remote printers or storage devices through      intervening private or public networks. Modems, cable modem and Ethernet      cards are just a few of the currently available types of network      adapters.
Patent applications by  Curtis E. Hrischuk, Holly Springs, NC US
Patent applications by  Thomas E. Creamer, Boca Raton, FL US
Patent applications by International Business Machines Corporation
Patent applications in class  Memory configuring
Patent applications in all subclasses  Memory configuring
User Contributions:
Comment about this patent or add new information about this topic:
Public Comment:Â Â (50-4000 characters)
